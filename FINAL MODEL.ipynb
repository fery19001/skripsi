{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "337071a0-7501-40f6-b883-62dbb4d95263",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.style as style\n",
    "import matplotlib.colors as mcolors\n",
    "import ast\n",
    "import pyLDAvis.gensim_models as gensimvis\n",
    "import pickle \n",
    "import pyLDAvis\n",
    "\n",
    "from pprint import pprint\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "from collections import Counter\n",
    "from translate import Translator\n",
    "from pysentimiento import create_analyzer\n",
    "from geneticalgorithm2 import geneticalgorithm2 as ga\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from transformers import pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from gensim import corpora, models, similarities\n",
    "from gensim.models import LdaMulticore, CoherenceModel\n",
    "from gensim.utils import ClippedCorpus\n",
    "from nltk.tokenize import word_tokenize\n",
    "from yellowbrick.cluster.elbow import KElbowVisualizer\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1697d7bf-5e34-4135-b83d-529a74c88f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "unstemmed = pd.read_csv('./data/unstemmed_REVISI.csv')\n",
    "stemmed = pd.read_csv('./data/stemmed_REVISI.csv')\n",
    "ndata = pd.read_csv('./data/data_non_duplicate_tweets_REVISI.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e604356f-6f66-4be9-b5a6-e728a2f2112d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def add_hashtag(hashtags):\n",
    "#     return [\"#\" + hashtag for hashtag in ast.literal_eval(hashtags)]\n",
    "\n",
    "def text_with_hashtag(texts, hashtags):\n",
    "    return texts + \" \" + pd.DataFrame([\" \".join(ast.literal_eval(x)) for x in hashtags])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b47909ed-e061-42f3-aaa1-a21ecf50a845",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stemmed['hashtags_added'] = stemmed.hashtags.apply(add_hashtag)\n",
    "stemmed['hashtags_joined'] = stemmed.hashtags.apply(lambda x: \" \".join(ast.literal_eval(x)))\n",
    "# stemmed['hashtags_count'] = stemmed.hashtags.map(len)\n",
    "\n",
    "# unstemmed['hashtags_added'] = unstemmed.hashtags.apply(add_hashtag)\n",
    "unstemmed['hashtags_joined'] = unstemmed.hashtags.apply(lambda x: \" \".join(ast.literal_eval(x)))\n",
    "# unstemmed['hashtags_count'] = unstemmed.hashtags.map(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34119b9d-6e17-4fd0-be85-c65672699546",
   "metadata": {},
   "outputs": [],
   "source": [
    "unstemmed.dropna(subset=['text'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "875e046b-646b-4470-8154-246ee0c8c0c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(stemmed), len(unstemmed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d581d23-3b07-4309-9585-67185011e54d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# views = stemmed[['text', 'hashtags_joined', 'text_hashtag', 'hashtags_count', 'users.username']]\n",
    "views = pd.DataFrame()\n",
    "views['unstemmed_text'] = unstemmed.text.tolist()\n",
    "views['stemmed_text'] = stemmed.text.tolist()\n",
    "views['created_at'] = unstemmed.created_at.tolist()\n",
    "views['hashtags'] = stemmed.hashtags.tolist()\n",
    "views['users'] = unstemmed['users.username'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b46a7a63-c505-45c4-b6e5-c5d90615657b",
   "metadata": {},
   "source": [
    "##### Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e53dae61-b94e-4f9f-b894-d076dab5240a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentimentdf = pd.DataFrame()\n",
    "sentimentdf['text'] = ndata.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6e11903-b865-4375-8b26-329bb065b6c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://chatbotslife.com/indonesian-twitter-sentiment-analysis-using-pretrained-neural-network-transformer-bert-97ca96a4aa60\n",
    "pretrained_id = \"mdhugol/indonesia-bert-sentiment-classification\"\n",
    "label_id = {'LABEL_0': 'positive', 'LABEL_1': 'neutral', 'LABEL_2': 'negative'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66b6ec36-e0ba-49be-a828-f73365613814",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(pretrained_id)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(pretrained_id)\n",
    "sentiment_analysis = pipeline(\"sentiment-analysis\", model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "157cdff8-3539-4217-ad10-6d5cd01c1e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "statuses = []\n",
    "scores = []\n",
    "for st in tqdm(sentimentdf.text):\n",
    "    sentiment = sentiment_analysis(st)\n",
    "    status = label_id[sentiment[0]['label']]\n",
    "    score = sentiment[0]['score']\n",
    "    statuses.append(status)\n",
    "    scores.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09ec361e-141d-4f4d-bcf8-07d669f6e893",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentimentdf['sentiment'] = statuses\n",
    "sentimentdf['sentiment_scores'] = scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4562d00f-f287-4c35-9804-b2d2173d27dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentimentdf.sentiment.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5a86f0d-9099-47fc-868c-3662fcbd1e02",
   "metadata": {},
   "source": [
    "##### Emotion Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9b94ab3-1b5e-46eb-b2f7-a04bdd0818d8",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "pretrained_name = \"StevenLimcorn/indonesian-roberta-base-emotion-classifier\"\n",
    "nlp = pipeline(\n",
    "    \"sentiment-analysis\",\n",
    "    model=pretrained_name,\n",
    "    tokenizer=pretrained_name\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "280b72c3-9d96-40e3-a382-610742a14bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "emotions = []\n",
    "emotions_scores = []\n",
    "for st in tqdm(sentimentdf.text):\n",
    "    emotion = nlp(st)[0]['label']\n",
    "    score = nlp(st)[0]['score']\n",
    "    emotions_scores.append(score)\n",
    "    emotions.append(emotion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "166b1efe-049c-4934-b48c-df70e6226fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentimentdf['emotions'] = emotions\n",
    "sentimentdf['emotions_scores'] = emotions_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2936655-50d4-4fc0-ae56-d1ab872e1f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentimentdf.emotions.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "415e142b-08e1-4cfc-9725-45bac885ed97",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentimentdf.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97213686-7516-46b4-b7f8-39bc01cbde7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentimentdf.to_csv('sentimentdf.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2cfa993-5bf4-4e8b-83bc-cc96d19f9b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentimentdf = pd.read_csv('sentimentdf.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ba3ba4-f16d-4140-a185-1590c858bdd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentimentdf.drop(columns=['Unnamed: 0'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a476d531-068a-423a-bb43-4de078fdd529",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentimentdf.emotions.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1a8233b-08cc-4fcb-a5db-c8e8fa2c4bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentimentdf.sentiment.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90620a9e-ea19-4f2b-adea-4d9f73b16339",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentimentdf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ade01401-b5aa-48db-af42-14638294a247",
   "metadata": {},
   "source": [
    "//"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "407e72f5-4eaa-41e8-9de5-cad46f7f75b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "views.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bbe7f56-ef25-46af-aa0f-453c7a5f803d",
   "metadata": {},
   "source": [
    "# TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "416d43c4-3ed3-484c-9a3c-336167612c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_tfIdf(array):\n",
    "    max_features = len(array)\n",
    "\n",
    "    # calc TF vector\n",
    "    cvect = CountVectorizer(max_features=max_features)\n",
    "    TF_vector = cvect.fit_transform(array)\n",
    "\n",
    "    # normalize TF vector\n",
    "    normalized_TF_vector = normalize(TF_vector, norm='l1', axis=1)\n",
    "\n",
    "    # calc IDF\n",
    "    tfidf = TfidfVectorizer(max_features=max_features, smooth_idf=False)\n",
    "    tfs = tfidf.fit_transform(array)\n",
    "    IDF_vector = tfidf.idf_\n",
    "\n",
    "    # hitung TF x IDF sehingga dihasilkan TFIDF matrix / vector\n",
    "    tfidf_mat = normalized_TF_vector.multiply(IDF_vector).toarray()\n",
    "    \n",
    "    terms = tfidf.get_feature_names_out()\n",
    "\n",
    "    # sum tfidf frequency of each term through documents\n",
    "    sums = tfidf_mat.sum(axis=0)\n",
    "\n",
    "    # connecting term to its sums frequency\n",
    "    data = []\n",
    "    for col, term in enumerate(terms):\n",
    "        data.append((term, np.round(sums[col]) ))\n",
    "\n",
    "    ranking = pd.DataFrame(data, columns=['term','rank'])\n",
    "    ranking.sort_values('rank', ascending=False, inplace=True)\n",
    "    ranking.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    return tfs, terms, tfidf, tfidf_mat, ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5599588-97ae-4c58-a1bc-e53943856f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "unstemmed_text_tfs, unstemmed_text_terms, unstemmed_text_tfidf, unstemmed_text_tfidf_mat, unstemmed_text_ranking = generate_tfIdf(list(views.unstemmed_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ca18f0a-09a4-4c31-91fd-efdcb5cff10b",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmed_text_tfs, stemmed_text_terms, stemmed_text_tfidf, stemmed_text_tfidf_mat, stemmed_text_ranking = generate_tfIdf(list(views.stemmed_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10948267-1521-4957-afaf-32e05d117e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "hashtag_tfs, hashtag_terms, hashtag_tfidf, hashtag_tfidf_mat, hashtag_ranking = generate_tfIdf(list(views.hashtags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7107a5b-e9d5-4248-a5a5-255d80508f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmed_text_dist = 1 - cosine_similarity(stemmed_text_tfidf_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "126a9941-f3f3-4239-b0f4-9f1e1807df87",
   "metadata": {},
   "outputs": [],
   "source": [
    "unstemmed_text_dist = 1 - cosine_similarity(unstemmed_text_tfidf_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b8737ea-dc1b-4ffc-8f38-fbe7a847a1b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "hastag_dist = 1 - cosine_similarity(hashtag_tfidf_mat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a5cf607-2430-455f-a77e-0d6ba4d2ef64",
   "metadata": {},
   "source": [
    "##### TF-IDF for Hashtags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c39ac2e3-9cf4-4381-a638-27876f150edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# small sample\n",
    "\n",
    "# TODO https://smyachenkov.com/posts/categorizing-instagram-tags-with-k-means/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3d72495-f9dd-432a-b083-a6a950dc1e1b",
   "metadata": {},
   "source": [
    "##### Hashtags with the most appearances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5170efe-9371-4c25-9b27-56fb16e2386b",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "hashtag_ranking[:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "321af2f6-983a-4e76-97a1-714489e87dec",
   "metadata": {},
   "source": [
    "# Hashtags K-Means Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "094c9f46-3ec0-4532-8631-2a241f4cd5b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# elbow method to define cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "802dba38-85ed-4e16-84cc-c06ea5f38dd7",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = KMeans(random_state=42, n_init=1, init='k-means++', verbose=True, max_iter=5)\n",
    "visualizer = KElbowVisualizer(model, k=(2,15))\n",
    "\n",
    "visualizer.fit(hashtag_tfs.toarray())        # Fit the data to the visualizer\n",
    "visualizer.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb52ab8-f317-427b-9883-7179404eafac",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cluster_hashtag = 3\n",
    "km_hashtag = KMeans(num_cluster_hashtag, random_state=123, n_init=1, init='k-means++', verbose=True, max_iter=5)\n",
    "km_hashtag.fit(hashtag_tfs)\n",
    "y_km = km_hashtag.predict(hashtag_tfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c813639d-c3ca-4f4c-a1fd-1255a472f609",
   "metadata": {},
   "outputs": [],
   "source": [
    "hashtag_clusters = km_hashtag.labels_.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1543df4e-3688-406e-9638-09776dc5605c",
   "metadata": {},
   "outputs": [],
   "source": [
    "views['hashtag_clusters'] = hashtag_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49376b6a-6e72-4487-9273-1c68ce3a54ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "hashtag_feature_name = hashtag_tfidf.get_feature_names_out()\n",
    "hashtag_top_features = 30\n",
    "hashtag_ordered_centroid = km_hashtag.cluster_centers_.argsort()[:,::-1]\n",
    "\n",
    "hashtag_clusters = []\n",
    "hashtag_key_features = []\n",
    "final_hashtags = []\n",
    "for cluster in range(num_cluster_hashtag):\n",
    "    hashtag_key_feature = [hashtag_feature_name[index] for index in hashtag_ordered_centroid[cluster,:hashtag_top_features]]\n",
    "    hashtag_cluster = views[views['hashtag_clusters']==cluster]['hashtags'].values.tolist()\n",
    "    hashtag_clusters.append(str(cluster+1))\n",
    "    hashtag_key_features.append(hashtag_key_feature)\n",
    "    final_hashtags.append(hashtag_cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9689c2c3-c590-4dea-a95e-b08e998e12c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_hashtags_count = [len(ht) for ht in final_hashtags] \n",
    "final_hashtags_clusters = []\n",
    "final_hashtags_key_features = []\n",
    "for x, cluster in enumerate(hashtag_clusters):\n",
    "    for count in range(final_hashtags_count[x]):\n",
    "        final_hashtags_clusters.append(cluster)\n",
    "for y, key in enumerate(hashtag_key_features):\n",
    "    for count in range(final_hashtags_count[y]):\n",
    "        final_hashtags_key_features.append(key)\n",
    "final_hashtags_1 = []\n",
    "for hashtag in final_hashtags:\n",
    "    for ht in hashtag:\n",
    "        final_hashtags_1.append(ht)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "835f7100-4192-44da-9d7e-7b8586832820",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Tweets count each cluster: \\n\")\n",
    "for i in range(len(hashtag_clusters)):\n",
    "    print(f\"Cluster {i+1}: {final_hashtags_count[i]}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdcf7478-9133-4165-9625-bbdaeff3d99f",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame([final_hashtags_clusters, final_hashtags_key_features, final_hashtags_1]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3505f2a-348b-47d9-9e9a-87b1f97e2365",
   "metadata": {},
   "outputs": [],
   "source": [
    "results.columns = ['cluster', 'key_features', 'hashtag']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f49063-d8c7-4a54-97be-dfa19e8a4928",
   "metadata": {},
   "outputs": [],
   "source": [
    "results.key_features.apply(str).unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9137fde-1311-41aa-9f74-742090f78611",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_2 = [ast.literal_eval(tr2) for tr2 in results.key_features.apply(str).unique().tolist()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d41e4e80-6a27-48cd-8e3e-da61bf767f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# results_2[1].pop(1)\n",
    "# results_2[3].pop(0)\n",
    "# results_2[3].pop(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3456a2e3-e986-4bb0-8147-6a213525e62c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, c in enumerate([\" \".join(joined) for joined in results_2]):\n",
    "    print(f\"Cluster {i+1}: {c}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f97515b-a287-4bea-804d-a086f2cdc92b",
   "metadata": {},
   "source": [
    "# Text Topic Modelling using LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30fc769f-0b5c-4099-ba01-993acd123d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmdata = pd.read_csv('./data/nddataTopicModelling.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24e514de-0f97-447a-9e21-20f5d688413f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmdata.dropna(subset=['text'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1d872e2-6721-4dd4-8105-4395187f3b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_text = [word_tokenize(text) for text in tmdata.text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aaa63c6-ca58-4a33-ba61-b1682d1afd19",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a Gensim dictionary from the texts\n",
    "dictionary = corpora.Dictionary(tokenized_text)\n",
    "\n",
    "#remove extremes (similar to the min/max df step used when creating the tf-idf matrix)\n",
    "dictionary.filter_extremes(no_below=1, no_above=0.8)\n",
    "\n",
    "#convert the dictionary to a bag of words corpus for reference\n",
    "corpus = [dictionary.doc2bow(text) for text in tokenized_text]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "079a987e-c39a-420b-8a2e-e3d6b92845ea",
   "metadata": {},
   "source": [
    "##### Baseline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a652b27b-b79f-4f8f-b2a2-e27f602d5e43",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Build LDA model\n",
    "lda_model = LdaMulticore(corpus=corpus,\n",
    "                         id2word=dictionary,\n",
    "                         num_topics=5, \n",
    "                         random_state=100,\n",
    "                         chunksize=100,\n",
    "                         passes=10,\n",
    "                         per_word_topics=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11e56282-d0dd-44c5-9e4b-d07ac72dcc5e",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Print the Keyword in the 10 topics\n",
    "pprint(lda_model.print_topics())\n",
    "doc_lda = lda_model[corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b55079bf-ec2c-46af-b06f-27c8395c8e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Coherence Score\n",
    "coherence_model_lda = CoherenceModel(model=lda_model, texts=tokenized_text, dictionary=dictionary, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('Coherence Score: ', coherence_lda)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75a10d30-eeee-4404-aadb-4435fdce4739",
   "metadata": {},
   "source": [
    "##### Hyperparameter tuning\n",
    "https://towardsdatascience.com/evaluate-topic-model-in-python-latent-dirichlet-allocation-lda-7d57484bb5d0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d7a0c86-41a3-4c6e-859d-20c0473503a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_coherence_values(corpus, dictionary, k, a, b):\n",
    "    \n",
    "    lda_model = LdaMulticore(corpus=corpus,\n",
    "                             id2word=dictionary,\n",
    "                             num_topics=k, \n",
    "                             random_state=100,\n",
    "                             chunksize=100,\n",
    "                             passes=10,\n",
    "                             alpha=a,\n",
    "                             eta=b)\n",
    "    \n",
    "    coherence_model_lda = CoherenceModel(model=lda_model, texts=tokenized_text, dictionary=dictionary, coherence='c_v')\n",
    "    \n",
    "    return coherence_model_lda.get_coherence()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4de9727c-b4f3-4705-9076-25f00586ceea",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# grid = {}\n",
    "# grid['Validation_Set'] = {}\n",
    "\n",
    "# # Topics range\n",
    "# min_topics = 2\n",
    "# max_topics = 11\n",
    "# step_size = 1\n",
    "# topics_range = range(min_topics, max_topics, step_size)\n",
    "\n",
    "# # Alpha parameter\n",
    "# alpha = list(np.arange(0.01, 1, 0.3))\n",
    "# alpha.append('symmetric')\n",
    "# alpha.append('asymmetric')\n",
    "\n",
    "# # Beta parameter\n",
    "# beta = list(np.arange(0.01, 1, 0.3))\n",
    "# beta.append('symmetric')\n",
    "\n",
    "# # Validation sets\n",
    "# num_of_docs = len(corpus)\n",
    "# corpus_sets = [ClippedCorpus(corpus, int(num_of_docs*0.75)), \n",
    "#                corpus]\n",
    "\n",
    "# corpus_title = ['75% Corpus', '100% Corpus']\n",
    "\n",
    "# model_results = {'Validation_Set': [],\n",
    "#                  'Topics': [],\n",
    "#                  'Alpha': [],\n",
    "#                  'Beta': [],\n",
    "#                  'Coherence': []\n",
    "#                 }\n",
    "\n",
    "# # Can take a long time to run\n",
    "# if 1 == 1:\n",
    "#     pbar = tqdm(total=(len(beta)*len(alpha)*len(topics_range)*len(corpus_title)))\n",
    "    \n",
    "#     # iterate through validation corpuses\n",
    "#     for i in range(len(corpus_sets)):\n",
    "#         # iterate through number of topics\n",
    "#         for k in topics_range:\n",
    "#             # iterate through alpha values\n",
    "#             for a in alpha:\n",
    "#                 # iterare through beta values\n",
    "#                 for b in beta:\n",
    "#                     # get the coherence score for the given parameters\n",
    "#                     cv = compute_coherence_values(corpus=corpus_sets[i], dictionary=dictionary, \n",
    "#                                                   k=k, a=a, b=b)\n",
    "#                     # Save the model results\n",
    "#                     model_results['Validation_Set'].append(corpus_title[i])\n",
    "#                     model_results['Topics'].append(k)\n",
    "#                     model_results['Alpha'].append(a)\n",
    "#                     model_results['Beta'].append(b)\n",
    "#                     model_results['Coherence'].append(cv)\n",
    "                    \n",
    "#                     pbar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41f830cf-213d-4ffe-bdd6-ce6127610966",
   "metadata": {},
   "outputs": [],
   "source": [
    "    # pd.DataFrame(model_results).to_csv('./data/lda_tuning_results.csv', index=False)\n",
    "    # pbar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84bf4f85-78ce-45da-9e68-8087d9b80aa5",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_topics = pd.read_csv('./data/lda_tuning_results.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf5d304-34f8-49ae-9b13-e1a7636efad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "coh_values = [df_topics[df_topics.Topics == value].Coherence.values[0] for value in df_topics[\"Topics\"].unique().tolist()]\n",
    "df_num_topics = df_topics.Topics.unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a398ffb-a75a-4b9d-be30-1e714277b78b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(df_num_topics, coh_values)\n",
    "\n",
    "ax.set(xlabel='Alpha', ylabel='Coherence',\n",
    "       title='Topic Coherence: Determining optimal number of topics')\n",
    "ax.grid()\n",
    "\n",
    "# fig.savefig(\"topic_coherence.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c8a3d29-3870-4d21-bf16-f5b7af68b860",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "max(df_topics[df_topics.Topics == 4].Coherence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59a8ea33-9b7d-4436-9e5e-cf4b4371ab15",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_topics[df_topics.Topics == 4] #alpha = 0.01, beta=0.61\n",
    "#alpha = 0.0909, beta= 0.61"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8f6b3fd-8157-466a-aa34-e4e6a0611ada",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d7971cc9-4e7d-461b-9c02-6bd8177b48ac",
   "metadata": {},
   "source": [
    "##### Final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a97422c-2ad7-41d3-88a5-e38947feaf7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_topics=4\n",
    "lda_model = LdaMulticore(corpus=corpus,\n",
    "                         id2word=dictionary,\n",
    "                         num_topics=num_topics, \n",
    "                         random_state=100,\n",
    "                         chunksize=100,\n",
    "                         passes=10,\n",
    "                         alpha=0.01,\n",
    "                         eta=0.61)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adec8904-a45c-4ba0-ba8f-95f3d28f12a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# topics_matrix\n",
    "topics_matrix = lda_model.show_topics(formatted=False, num_words=30)\n",
    "topics_matrix = np.array(topics_matrix, dtype=object)\n",
    "\n",
    "topic_words = topics_matrix[:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ee5d21d-8218-4025-b780-8d7abc212f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(lda_model.print_topics())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "327914b8-24da-4f6c-be67-71cb2939d983",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in topic_words:\n",
    "    print([str(word) for word in i])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "381f3fbf-9f25-4e6e-af1a-a2840ffbc821",
   "metadata": {},
   "source": [
    "##### What is the Dominant topic and its percentage contribution in each document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcca2621-b238-47a6-a7c7-489ddc93b6da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_topics_sentences(ldamodel, corpus, texts):\n",
    "    # Init output\n",
    "    sent_topics_df = pd.DataFrame()\n",
    "\n",
    "    # Get main topic in each document\n",
    "    for i, row_list in tqdm(enumerate(ldamodel[corpus])):\n",
    "        row = row_list[0] if ldamodel.per_word_topics else row_list            \n",
    "        # print(row)\n",
    "        row = sorted(row, key=lambda x: (x[1]), reverse=True)\n",
    "        # Get the Dominant topic, Perc Contribution and Keywords for each document\n",
    "        for j, (topic_num, prop_topic) in enumerate(row):\n",
    "            if j == 0:  # => dominant topic\n",
    "                wp = ldamodel.show_topic(topic_num)\n",
    "                topic_keywords = \", \".join([word for word, prop in wp])\n",
    "                sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n",
    "            else:\n",
    "                break\n",
    "    sent_topics_df.columns = ['Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords']\n",
    "\n",
    "    # Add original text to the end of the output\n",
    "    contents = pd.Series(texts)\n",
    "    sent_topics_df = pd.concat([sent_topics_df, contents], axis=1)\n",
    "    return(sent_topics_df)\n",
    "\n",
    "\n",
    "df_topic_sents_keywords = format_topics_sentences(lda_model, corpus, tokenized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8d8b945-4604-4086-80c4-c8594177b8c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format\n",
    "df_dominant_topic = df_topic_sents_keywords.reset_index()\n",
    "df_dominant_topic.columns = ['Document_No', 'Dominant_Topic', 'Topic_Perc_Contrib', 'Keywords', 'Text']\n",
    "df_dominant_topic.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e315acb5-df62-4c90-87c4-057542fd2e92",
   "metadata": {},
   "source": [
    "##### The most representative sentence for each topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "845c1fca-bc3b-4377-8108-2f5779b40e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display setting to show more characters in column\n",
    "pd.options.display.max_colwidth = 100\n",
    "\n",
    "sent_topics_sorteddf_mallet = pd.DataFrame()\n",
    "sent_topics_outdf_grpd = df_topic_sents_keywords.groupby('Dominant_Topic')\n",
    "\n",
    "for i, grp in sent_topics_outdf_grpd:\n",
    "    sent_topics_sorteddf_mallet = pd.concat([sent_topics_sorteddf_mallet, \n",
    "                                             grp.sort_values(['Perc_Contribution'], ascending=False).head(1)], \n",
    "                                            axis=0)\n",
    "\n",
    "# Reset Index    \n",
    "sent_topics_sorteddf_mallet.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Format\n",
    "sent_topics_sorteddf_mallet.columns = ['Topic_Num', \"Topic_Perc_Contrib\", \"Keywords\", \"Representative Text\"]\n",
    "\n",
    "# Show\n",
    "sent_topics_sorteddf_mallet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33fe487d-6d86-4a2e-bcc4-b0f51bff6d8c",
   "metadata": {},
   "source": [
    "##### Frequency Distribution of Word Counts in Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0e22c00-8f0c-4319-ae98-c231bab50f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_lens = [len(d) for d in df_dominant_topic.Text]\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(16,7), dpi=160)\n",
    "plt.hist(doc_lens, bins = 20, color='navy')\n",
    "plt.text(30, 10000, \"Mean   : \" + str(round(np.mean(doc_lens))))\n",
    "plt.text(30, 9000, \"Median : \" + str(round(np.median(doc_lens))))\n",
    "plt.text(30, 8000, \"Stdev   : \" + str(round(np.std(doc_lens))))\n",
    "plt.text(30, 7000, \"1%ile    : \" + str(round(np.quantile(doc_lens, q=0.01))))\n",
    "plt.text(30, 6000, \"99%ile  : \" + str(round(np.quantile(doc_lens, q=0.99))))\n",
    "\n",
    "plt.gca().set(xlim=(0, 40), ylabel='Number of Documents', xlabel='Document Word Count')\n",
    "plt.tick_params(size=16)\n",
    "plt.title('Distribution of Document Word Counts', fontdict=dict(size=22))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f12a373b-59a0-4c2f-99c4-8df5533ad374",
   "metadata": {},
   "source": [
    "##### Word Counts of Topic Keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "144e07f4-e650-4eb1-8d1f-a50579694a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "temptopics = lda_model.show_topics(formatted=False, num_words=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdfb8ae0-0a16-458f-a8bc-7632ab11ed6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic1 = [temptopics[0][1][0], temptopics[0][1][1], ('uuciptakerja', 0.008051984), temptopics[0][1][3], \n",
    "         temptopics[0][1][4], temptopics[0][1][6], temptopics[0][1][7], temptopics[0][1][8], temptopics[0][1][9], temptopics[0][1][13]]\n",
    "topic2 = [temptopics[1][1][0], temptopics[1][1][2], ('phk', 0.0132331895), temptopics[1][1][7], temptopics[1][1][8], temptopics[1][1][11],\n",
    "         temptopics[1][1][14], temptopics[1][1][16], temptopics[1][1][20], temptopics[1][1][22]]\n",
    "topic3 = [temptopics[2][1][0], temptopics[2][1][1], temptopics[2][1][2], temptopics[2][1][4], temptopics[2][1][5], temptopics[2][1][6],\n",
    "         temptopics[2][1][7], temptopics[2][1][8], temptopics[2][1][14], temptopics[2][1][19]]\n",
    "topic4 = [temptopics[3][1][0], temptopics[3][1][1], temptopics[3][1][4], temptopics[3][1][6], temptopics[3][1][7], temptopics[3][1][9],\n",
    "         temptopics[3][1][10], temptopics[3][1][13], temptopics[3][1][17], temptopics[3][1][28]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f89888cc-ad71-464a-aa06-dcb40c5f8648",
   "metadata": {},
   "outputs": [],
   "source": [
    "temptopics = [(0,\n",
    "  topic1),\n",
    " (1,\n",
    "  topic2),\n",
    " (2,\n",
    "  topic3),\n",
    " (3,\n",
    "  topic4)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22c90836-2150-4b87-9570-bd07cd3b9c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# topics = lda_model.show_topics(formatted=False)\n",
    "topics = [(0,\n",
    "  topic1),\n",
    " (1,\n",
    "  topic2),\n",
    " (2,\n",
    "  topic3),\n",
    " (3,\n",
    "  topic4)]\n",
    "data_flat = [w for w_list in tokenized_text for w in w_list]\n",
    "counter = Counter(data_flat)\n",
    "\n",
    "out = []\n",
    "for i, topic in topics:\n",
    "    for word, weight in topic:\n",
    "        out.append([word, i , weight, counter[word]])\n",
    "\n",
    "df = pd.DataFrame(out, columns=['word', 'topic_id', 'importance', 'word_count'])        \n",
    "\n",
    "# Plot Word Count and Weights of Topic Keywords\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16,10), sharey=True, dpi=160)\n",
    "cols = [color for name, color in mcolors.TABLEAU_COLORS.items()]\n",
    "for i, ax in enumerate(axes.flatten()):\n",
    "    ax.bar(x='word', height=\"word_count\", data=df.loc[df.topic_id==i, :], color=cols[i], width=0.5, alpha=0.3, label='Word Count')\n",
    "    ax_twin = ax.twinx()\n",
    "    ax_twin.bar(x='word', height=\"importance\", data=df.loc[df.topic_id==i, :], color=cols[i], width=0.2, label='Weights')\n",
    "    ax.set_ylabel('Word Count', color=cols[i])\n",
    "    ax.set_title('Topic: ' + str(i+1), color=cols[i], fontsize=16)\n",
    "    ax.tick_params(axis='y', left=False)\n",
    "    ax.set_xticklabels(df.loc[df.topic_id==i, 'word'], rotation=30, horizontalalignment= 'right')\n",
    "    ax.legend(loc='upper left'); ax_twin.legend(loc='upper right')\n",
    "\n",
    "fig.tight_layout(w_pad=2)    \n",
    "fig.suptitle('Word Count and Importance of Topic Keywords', fontsize=22, y=1.05)    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e946cea-14bd-4572-adc8-ec3456a655a5",
   "metadata": {},
   "source": [
    "##### pyLDAvis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41a8a196-5a83-47c7-8e66-12f8a019df59",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Visualize the topics\n",
    "pyLDAvis.enable_notebook()\n",
    "\n",
    "LDAvis_data_filepath = os.path.join('./results/ldavis_tuned_'+str(num_topics))\n",
    "\n",
    "# # this is a bit time consuming - make the if statement True\n",
    "# # if you want to execute visualization prep yourself\n",
    "if 1 == 1:\n",
    "    LDAvis_prepared = gensimvis.prepare(lda_model, corpus, dictionary)\n",
    "    with open(LDAvis_data_filepath, 'wb') as f:\n",
    "        pickle.dump(LDAvis_prepared, f)\n",
    "\n",
    "# load the pre-prepared pyLDAvis data from disk\n",
    "with open(LDAvis_data_filepath, 'rb') as f:\n",
    "    LDAvis_prepared = pickle.load(f)\n",
    "\n",
    "pyLDAvis.save_html(LDAvis_prepared, './results/ldavis_tuned_'+ str(num_topics) +'.html')\n",
    "\n",
    "LDAvis_prepared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfabd54f-d632-4cc1-afc5-c614301b4f58",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
